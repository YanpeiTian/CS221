{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2048(nn.Module):\n",
    "    def __init__(self, input_size=16, filter1=512, filter2=4096, drop_prob=0.):\n",
    "        super(NN2048, self).__init__()\n",
    "        self.conv_a = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(2,1), padding=0)\n",
    "        self.conv_b = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(1,2), padding=0)\n",
    "        self.conv_aa = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_ab = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        self.conv_ba = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_bb = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.W_aa = nn.Linear(filter2 * 8, 1)\n",
    "        self.W_ab = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_ba = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_bb = nn.Linear(filter2 * 8, 1)\n",
    "\n",
    "    def flatten(self, x):\n",
    "        N = x.size()[0]\n",
    "        return x.view(N, -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        a = self.relu(self.conv_a(x))\n",
    "        b = self.relu(self.conv_b(x))\n",
    "        aa = self.flatten(self.relu(self.conv_aa(a)))\n",
    "        ab = self.flatten(self.relu(self.conv_ab(a)))\n",
    "        ba = self.flatten(self.relu(self.conv_ba(b)))\n",
    "        bb = self.flatten(self.relu(self.conv_bb(b)))\n",
    "        out = self.W_aa(aa) + self.W_ab(ab) + self.W_ba(ba) + self.W_bb(bb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(grid):\n",
    "    r = np.zeros(shape=(16, 4, 4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            r[grid[i, j],i, j]=1\n",
    "    return r\n",
    "\n",
    "def add_two(mat):\n",
    "    indexs=np.argwhere(mat==0)\n",
    "    index=np.random.randint(0,len(indexs))\n",
    "    mat[tuple(indexs[index])] = 1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleScore=[0,0,4,16,48,128,320,768,1792,4096,9216,20480,45056,98304,212992,458752,983040]\n",
    "moveDict=np.load('move.npy')\n",
    "\n",
    "def move(list):\n",
    "    return moveDict[list[0],list[1],list[2],list[3],:]\n",
    "\n",
    "def lookup(x):\n",
    "    return singleScore[x]\n",
    "\n",
    "lookup = np.vectorize(lookup)\n",
    "\n",
    "def getScore(matrix):\n",
    "    return np.sum(lookup(matrix))\n",
    "\n",
    "def getMove(grid):\n",
    "    board_list = []\n",
    "    for i in range(4):\n",
    "        newGrid=moveGrid(grid, i)\n",
    "        if not isSame(grid,newGrid):\n",
    "            board_list.append((newGrid, i, getScore(newGrid)))\n",
    "    return board_list\n",
    "        \n",
    "def moveGrid(grid,i):\n",
    "    # new=np.zeros((4,4),dtype=np.int)\n",
    "    new = None\n",
    "    if i==0:\n",
    "        # move up\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==1:\n",
    "        # move left\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int)\n",
    "    elif i==2:\n",
    "        # move down\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==3:\n",
    "        # move right\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int)\n",
    "    return new\n",
    "\n",
    "def isSame(grid1,grid2):\n",
    "    return np.all(grid1==grid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vchange(grid, v):\n",
    "    g0 = grid\n",
    "    g1 = g0[:,::-1,:]\n",
    "    g2 = g0[:,:,::-1]\n",
    "    g3 = g2[:,::-1,:]\n",
    "    r0 = grid.swapaxes(1,2)\n",
    "    r1 = r0[:,::-1,:]\n",
    "    r2 = r0[:,:,::-1]\n",
    "    r3 = r2[:,::-1,:]\n",
    "    xtrain = np.array([g0,g1,g2,g3,r0,r1,r2,r3])\n",
    "    ytrain = np.array([v]*8)\n",
    "    return xtrain, ytrain\n",
    "\n",
    "def gen_sample_and_learn(model, optimizer, loss_fn, is_train = False, explorationProb=0.1):\n",
    "    model.eval()\n",
    "    game_len = 0\n",
    "    game_score = 0\n",
    "    last_grid1 = np.zeros((4,4),dtype=np.int)\n",
    "    last_grid1 = add_two(last_grid1)\n",
    "    last_grid2 = make_input(last_grid1)\n",
    "    last_loss = 0\n",
    "\n",
    "    while True:\n",
    "        grid_array = add_two(last_grid1)\n",
    "        board_list = getMove(grid_array)\n",
    "        if board_list:\n",
    "            boards = np.array([make_input(g) for g,m,s in board_list])\n",
    "            p = model(torch.from_numpy(boards).cuda()).flatten().detach()        \n",
    "            game_len += 1\n",
    "            best_v = None\n",
    "            for i, (g,m,s) in enumerate(board_list):\n",
    "                v = (s - game_score) + p[i].item()\n",
    "                if best_v is None or v > best_v:\n",
    "                    best_v = v\n",
    "                    best_score = s\n",
    "                    best_grid1 = board_list[i][0]\n",
    "                    best_grid2 = boards[i]\n",
    "                    \n",
    "        else:\n",
    "            best_v = 0\n",
    "            best_grid1 = None\n",
    "            best_grid2 = None\n",
    "            \n",
    "        if is_train:\n",
    "            x, y = Vchange(last_grid2, best_v)\n",
    "            x = torch.from_numpy(x).cuda()\n",
    "            y = torch.from_numpy(y).unsqueeze(dim=1).cuda().float()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)/2\n",
    "            last_loss = loss.item()\n",
    "            loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), 10.0) #\n",
    "            optimizer.step()\n",
    "            model.eval()\n",
    "#             if game_len % 50 == 0:\n",
    "#                 print (game_len, last_loss)\n",
    "                \n",
    "        if not board_list:\n",
    "            break\n",
    "            \n",
    "        # gibbs sampling or espilon-greedy\n",
    "        if is_train and random.random() < explorationProb:\n",
    "            idx = random.randint(0, len(board_list) - 1)\n",
    "            game_score = board_list[idx][2]\n",
    "            last_grid1 = board_list[idx][0]\n",
    "            last_grid2 = boards[idx]\n",
    "        else:\n",
    "            game_score = best_score\n",
    "            last_grid1 = best_grid1\n",
    "            last_grid2 = best_grid2\n",
    "        \n",
    "    return game_len, 2**grid_array.max(), game_score, last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-6\n",
    "beta1 = 0.8\n",
    "\n",
    "model = NN2048().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, 0.999))\n",
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 183 128 1708 48358292.0\n",
      "epoch 2 327 256 3820 9177346.0\n",
      "epoch 3 357 256 4136 4941937.0\n",
      "epoch 4 534 512 7276 7169395.0\n",
      "epoch 5 339 256 3932 3546295.0\n",
      "epoch 6 303 256 3548 2301029.0\n",
      "epoch 7 288 256 3276 2189338.5\n",
      "epoch 8 460 512 6160 2036827.5\n",
      "epoch 9 284 256 3216 2543565.5\n",
      "epoch 10 520 512 7152 1699343.25\n",
      "epoch 11 180 128 1692 1005748.3125\n",
      "epoch 12 405 256 4728 2928656.25\n",
      "epoch 13 295 256 3324 1410495.25\n",
      "epoch 14 228 128 2272 583761.375\n",
      "epoch 15 496 512 6712 2718500.0\n",
      "epoch 16 449 512 5912 1774196.0\n",
      "epoch 17 292 256 3280 1200197.25\n",
      "epoch 18 319 256 3596 904424.5625\n",
      "epoch 19 642 512 8404 2231533.0\n",
      "epoch 20 796 1024 12524 45923924.0\n",
      "epoch 21 332 256 3884 5655401.5\n",
      "epoch 22 874 1024 13420 11231558.0\n",
      "epoch 23 498 512 6556 9027185.0\n",
      "epoch 24 601 512 8104 6758472.0\n",
      "epoch 25 514 512 7120 7210343.0\n",
      "epoch 26 550 512 7420 5027830.0\n",
      "epoch 27 603 512 8124 7076512.0\n",
      "epoch 28 880 1024 13548 16244992.0\n",
      "epoch 29 231 128 2384 6075521.0\n",
      "epoch 30 664 512 8760 3068349.5\n",
      "epoch 31 1037 1024 16448 18808148.0\n",
      "epoch 32 486 512 6480 8918636.0\n",
      "epoch 33 474 512 6536 9544970.0\n",
      "epoch 34 597 512 8140 5469689.0\n",
      "epoch 35 1306 1024 20668 57446720.0\n",
      "epoch 36 902 1024 13848 34711076.0\n",
      "epoch 37 1486 2048 26188 119176656.0\n",
      "epoch 38 1026 1024 15948 55425016.0\n",
      "epoch 39 912 1024 14188 55909996.0\n",
      "epoch 40 916 1024 14700 72403128.0\n",
      "epoch 41 568 512 7708 21040456.0\n",
      "epoch 42 984 1024 15524 34652440.0\n",
      "epoch 43 828 1024 12800 21057918.0\n",
      "epoch 44 852 1024 13168 33516160.0\n",
      "epoch 45 545 512 7384 18339924.0\n",
      "epoch 46 631 512 8764 14732921.0\n",
      "epoch 47 1192 1024 20172 52852380.0\n",
      "epoch 48 567 512 7644 23246300.0\n",
      "epoch 49 1051 1024 16572 46126592.0\n",
      "epoch 50 478 512 6592 24863818.0\n",
      "epoch 51 547 512 7392 16803836.0\n",
      "epoch 52 928 1024 14804 30276928.0\n",
      "epoch 53 1056 1024 16636 29634500.0\n",
      "epoch 54 551 512 7472 16664824.0\n",
      "epoch 55 750 512 11008 28880516.0\n",
      "epoch 56 795 1024 12472 26978896.0\n",
      "epoch 57 728 1024 11420 25862720.0\n",
      "epoch 58 1042 1024 16476 25906008.0\n",
      "epoch 59 1022 1024 16260 43391512.0\n",
      "epoch 60 1281 2048 23500 129038352.0\n",
      "epoch 61 1066 1024 16748 52925552.0\n",
      "epoch 62 796 1024 12440 53386268.0\n",
      "epoch 63 791 1024 12412 46988056.0\n",
      "epoch 64 1043 1024 16492 44399300.0\n",
      "epoch 65 549 512 7416 25723542.0\n",
      "epoch 66 1033 1024 16416 50039824.0\n",
      "epoch 67 521 512 7112 26880384.0\n",
      "epoch 68 292 256 3308 13726791.0\n",
      "epoch 69 732 1024 11664 20603758.0\n",
      "epoch 70 1036 1024 16432 28357072.0\n",
      "epoch 71 811 1024 12652 32122504.0\n",
      "epoch 72 1014 1024 16076 34401360.0\n",
      "epoch 73 987 1024 15800 83690640.0\n",
      "epoch 74 593 512 8216 38624800.0\n",
      "epoch 75 540 512 7344 19695792.0\n",
      "epoch 76 1324 2048 23932 136491680.0\n",
      "epoch 77 444 256 5548 44740288.0\n",
      "epoch 78 1040 1024 16464 56291984.0\n",
      "epoch 79 276 256 3164 36401456.0\n",
      "epoch 80 784 1024 12364 29487300.0\n",
      "epoch 81 787 1024 12380 28927306.0\n",
      "epoch 82 543 512 7372 19036188.0\n",
      "epoch 83 1125 1024 17868 64423696.0\n",
      "epoch 84 424 512 5644 30265956.0\n",
      "epoch 85 1045 1024 16528 30597524.0\n",
      "epoch 86 673 1024 10704 38172672.0\n",
      "epoch 87 1050 1024 16556 29957558.0\n",
      "epoch 88 1050 1024 16556 80669968.0\n",
      "epoch 89 922 1024 14760 89228936.0\n",
      "epoch 90 814 1024 12620 59333440.0\n",
      "epoch 91 1049 1024 16608 84925648.0\n",
      "epoch 92 801 1024 12504 48735104.0\n",
      "epoch 93 606 512 8140 29011096.0\n",
      "epoch 94 542 512 7368 26983852.0\n",
      "epoch 95 562 512 7580 16596428.0\n",
      "epoch 96 1032 1024 16412 51284984.0\n",
      "epoch 97 997 1024 15852 45584888.0\n",
      "epoch 98 1033 1024 16416 60336012.0\n",
      "epoch 99 1036 1024 16444 78986856.0\n",
      "epoch 100 417 512 5584 48324860.0\n",
      "epoch 101 669 512 9656 28993976.0\n",
      "epoch 104 991 1024 15992 75892224.0\n",
      "epoch 105 169 128 1536 39055432.0\n",
      "epoch 106 834 1024 12892 33392984.0\n",
      "epoch 107 1050 1024 16556 60017624.0\n",
      "epoch 108 1008 1024 16016 51604032.0\n",
      "epoch 109 529 512 7248 39055272.0\n",
      "epoch 110 785 1024 12368 35109572.0\n",
      "epoch 111 961 1024 15504 64230188.0\n",
      "epoch 112 982 1024 15736 62021308.0\n",
      "epoch 113 531 512 7260 33880788.0\n",
      "epoch 114 544 512 7376 22410164.0\n",
      "epoch 115 398 512 5424 22009608.0\n",
      "epoch 116 796 1024 12464 15612204.0\n",
      "epoch 117 1027 1024 16348 36467864.0\n",
      "epoch 118 1141 1024 18060 74777144.0\n",
      "epoch 119 532 512 7264 44287808.0\n",
      "epoch 120 1504 2048 27148 176485472.0\n",
      "epoch 121 1005 1024 16000 112219384.0\n",
      "epoch 122 611 512 8924 99893512.0\n",
      "epoch 123 935 1024 14852 45929360.0\n",
      "epoch 126 1041 1024 16472 113184456.0\n",
      "epoch 127 568 512 7624 51505984.0\n",
      "epoch 128 1029 1024 16384 112065280.0\n",
      "epoch 129 1013 1024 16168 83499536.0\n",
      "epoch 130 931 1024 14828 97111936.0\n",
      "epoch 131 1327 2048 23948 147333776.0\n",
      "epoch 132 853 1024 13424 123587120.0\n",
      "epoch 133 1044 1024 16524 111465792.0\n",
      "epoch 134 814 1024 12620 53969896.0\n",
      "epoch 135 1047 1024 16512 66690756.0\n",
      "epoch 136 666 512 9116 44886584.0\n",
      "epoch 137 588 512 8236 41466256.0\n",
      "epoch 138 1056 1024 16600 42897800.0\n",
      "epoch 139 1422 2048 25920 335555328.0\n",
      "epoch 140 1439 2048 26060 285229056.0\n",
      "epoch 141 1533 2048 27568 198197184.0\n",
      "epoch 142 1132 1024 17904 129001272.0\n",
      "epoch 143 1259 2048 23144 291145216.0\n",
      "epoch 144 688 1024 10832 162905696.0\n",
      "epoch 145 1035 1024 16428 114889040.0\n",
      "epoch 146 925 1024 14784 151698784.0\n",
      "epoch 147 853 1024 13920 116576448.0\n",
      "epoch 148 531 512 7260 57327416.0\n",
      "epoch 149 599 512 8344 45397564.0\n",
      "epoch 150 576 512 7704 53019900.0\n",
      "epoch 151 1567 2048 27868 81375168.0\n",
      "epoch 152 1794 2048 32604 208067056.0\n",
      "epoch 153 1332 2048 24348 380011648.0\n",
      "epoch 154 869 1024 14064 183861920.0\n",
      "epoch 155 548 512 7424 102413936.0\n",
      "epoch 156 923 1024 14268 91589744.0\n",
      "epoch 157 1042 1024 16476 85458528.0\n",
      "epoch 158 915 1024 14684 122176128.0\n",
      "epoch 159 1043 1024 16516 66231828.0\n",
      "epoch 160 1169 1024 18776 96395536.0\n",
      "epoch 161 1017 1024 16200 87386624.0\n",
      "epoch 162 1040 1024 16464 83626552.0\n",
      "epoch 163 753 1024 11672 80898216.0\n",
      "epoch 164 936 1024 14940 57425252.0\n",
      "epoch 165 984 1024 15688 67891760.0\n",
      "epoch 166 975 1024 15688 128046848.0\n",
      "epoch 167 935 1024 14852 83534624.0\n",
      "epoch 168 1031 1024 16408 73909960.0\n",
      "epoch 169 901 1024 14504 76526032.0\n",
      "epoch 170 1537 2048 27600 191690848.0\n",
      "epoch 171 961 1024 15340 129420816.0\n",
      "epoch 172 1054 1024 16588 54283124.0\n",
      "epoch 173 1547 2048 27692 190717632.0\n",
      "epoch 174 481 512 6608 116700432.0\n",
      "epoch 175 1026 1024 16344 90867488.0\n",
      "epoch 176 982 1024 15724 100509256.0\n",
      "epoch 177 1933 2048 35136 215892768.0\n",
      "epoch 178 1066 1024 16700 138032896.0\n",
      "epoch 179 1036 1024 16444 115880016.0\n",
      "epoch 180 1554 2048 27752 301791648.0\n",
      "epoch 181 1564 2048 27844 232274752.0\n",
      "epoch 182 1522 2048 27416 365696320.0\n",
      "epoch 183 1645 2048 29176 315040800.0\n",
      "epoch 184 815 1024 12684 229599024.0\n",
      "epoch 185 2072 2048 37016 266984448.0\n",
      "epoch 186 1485 2048 26928 492145088.0\n",
      "epoch 187 582 512 7784 186168992.0\n",
      "epoch 188 1056 1024 16600 145314176.0\n",
      "epoch 189 1068 1024 16712 112232336.0\n",
      "epoch 190 172 128 1564 82272368.0\n",
      "epoch 191 475 512 6540 46316224.0\n",
      "epoch 192 1630 2048 28852 107545952.0\n",
      "epoch 193 1055 1024 16652 87464232.0\n",
      "epoch 194 1033 1024 16392 75626312.0\n",
      "epoch 195 630 512 8412 52713856.0\n",
      "epoch 196 1834 2048 32068 183522240.0\n",
      "epoch 197 1012 1024 16172 159093664.0\n",
      "epoch 198 531 512 7260 85396320.0\n",
      "epoch 199 485 256 5944 36617968.0\n",
      "epoch 200 1123 1024 17708 44810676.0\n",
      "epoch 201 557 512 7496 66121080.0\n",
      "epoch 202 727 1024 11292 71736928.0\n",
      "epoch 203 1682 2048 30044 208229872.0\n",
      "epoch 204 1000 1024 15880 131499440.0\n",
      "epoch 205 683 1024 10780 108288232.0\n",
      "epoch 206 1892 2048 33752 153143840.0\n",
      "epoch 207 1127 1024 17756 80661752.0\n",
      "epoch 208 1047 1024 16540 116422216.0\n",
      "epoch 209 1346 2048 24528 159305088.0\n",
      "epoch 210 902 1024 14396 163322640.0\n",
      "epoch 211 1047 1024 16540 74966864.0\n",
      "epoch 212 562 512 7580 74104240.0\n",
      "epoch 213 1032 1024 16412 92577088.0\n",
      "epoch 214 1016 1024 16220 88523016.0\n",
      "epoch 215 1691 2048 31148 297635136.0\n",
      "epoch 216 1584 2048 28056 184620576.0\n",
      "epoch 217 600 512 8072 171095968.0\n",
      "epoch 218 1935 2048 35148 301711808.0\n",
      "epoch 219 796 1024 12464 143402816.0\n",
      "epoch 220 1051 1024 16572 158910816.0\n",
      "epoch 221 1698 2048 31196 257173440.0\n",
      "epoch 222 652 1024 10532 141493616.0\n",
      "epoch 223 1032 1024 16412 113886232.0\n",
      "epoch 224 1073 1024 16796 73746736.0\n",
      "epoch 225 946 1024 15004 137799488.0\n",
      "epoch 226 1054 1024 16588 83730592.0\n",
      "epoch 227 1012 1024 16160 108185728.0\n",
      "epoch 228 1164 1024 18244 78064528.0\n",
      "epoch 229 1527 2048 27452 180646112.0\n",
      "epoch 230 1141 1024 17688 137557280.0\n",
      "epoch 231 832 1024 12776 97820752.0\n",
      "epoch 232 2047 2048 36812 160848816.0\n",
      "epoch 233 1333 2048 23992 203446000.0\n",
      "epoch 234 1070 1024 16724 144446624.0\n",
      "epoch 235 1542 2048 27628 306944480.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 236 1774 2048 32332 427115840.0\n",
      "epoch 237 1939 2048 35168 452629760.0\n",
      "epoch 238 1753 2048 32160 422012512.0\n",
      "epoch 239 597 512 7932 218271424.0\n",
      "epoch 240 1024 1024 16276 130852712.0\n",
      "epoch 241 1544 2048 27676 291618496.0\n",
      "epoch 242 599 512 8092 127791288.0\n",
      "epoch 243 1895 2048 34236 269127872.0\n",
      "epoch 244 1030 1024 16328 166544160.0\n",
      "epoch 245 549 512 7440 160299648.0\n",
      "epoch 246 799 1024 12492 87342488.0\n",
      "epoch 247 1061 1024 16664 70324096.0\n",
      "epoch 248 1517 2048 27252 320203584.0\n",
      "epoch 249 789 1024 12392 281971232.0\n",
      "epoch 250 1035 1024 16428 147069936.0\n",
      "epoch 251 1544 2048 27676 215336976.0\n",
      "epoch 252 991 1024 15820 184757152.0\n",
      "epoch 253 1552 2048 27740 135413408.0\n",
      "epoch 254 953 1024 15424 114659624.0\n",
      "epoch 255 1086 1024 17036 105984128.0\n",
      "epoch 256 1040 1024 16464 113278712.0\n",
      "epoch 257 1127 1024 18180 69017480.0\n",
      "epoch 258 1041 1024 16472 71477456.0\n",
      "epoch 259 1491 2048 26972 147666480.0\n",
      "epoch 260 1056 1024 16600 146684848.0\n",
      "epoch 261 1048 1024 16520 90861056.0\n",
      "epoch 262 558 512 7532 57712256.0\n",
      "epoch 263 1539 2048 27636 124114928.0\n",
      "epoch 264 993 1024 15832 102474160.0\n",
      "epoch 265 1533 2048 27512 147615904.0\n",
      "epoch 266 2053 2048 36880 258878512.0\n",
      "epoch 267 870 1024 13340 224301280.0\n",
      "epoch 268 1550 2048 27720 224921920.0\n",
      "epoch 269 443 512 5868 239164960.0\n",
      "epoch 270 1545 2048 27680 195468832.0\n",
      "epoch 271 709 512 10416 95445648.0\n",
      "epoch 272 1619 2048 28556 179225616.0\n",
      "epoch 273 1816 2048 32904 350704000.0\n",
      "epoch 274 1771 2048 31356 372261120.0\n",
      "epoch 275 787 1024 12380 275841024.0\n",
      "epoch 276 1790 2048 32648 334261600.0\n",
      "epoch 277 1889 2048 33800 252377984.0\n",
      "epoch 278 608 512 8400 246613536.0\n",
      "epoch 279 1038 1024 16456 190079248.0\n",
      "epoch 280 1618 2048 28552 185415888.0\n",
      "epoch 281 1032 1024 16384 199211152.0\n",
      "epoch 282 1519 2048 27388 253929280.0\n",
      "epoch 283 1582 2048 28044 326606016.0\n",
      "epoch 284 612 512 8208 194023984.0\n",
      "epoch 285 1326 2048 23884 155913248.0\n",
      "epoch 286 1555 2048 27756 118346984.0\n",
      "epoch 287 1022 1024 16260 120715664.0\n",
      "epoch 288 958 1024 15240 140994848.0\n",
      "epoch 289 442 512 5864 89791248.0\n",
      "epoch 290 875 1024 14304 136898112.0\n",
      "epoch 291 570 512 7660 66842040.0\n",
      "epoch 292 1453 2048 26216 109617136.0\n",
      "epoch 293 2090 2048 37244 174394432.0\n",
      "epoch 294 1049 1024 16552 142514208.0\n",
      "epoch 295 1053 1024 16584 113020400.0\n",
      "epoch 296 1027 1024 16300 136828576.0\n",
      "epoch 297 518 512 7084 76477760.0\n",
      "epoch 298 951 1024 14984 72035648.0\n",
      "epoch 299 601 512 8112 45161532.0\n",
      "epoch 300 1068 1024 16760 59056512.0\n",
      "epoch 301 1023 1024 16268 73901088.0\n",
      "epoch 302 1117 1024 17608 55244552.0\n",
      "epoch 303 2544 4096 51980 1418496000.0\n",
      "epoch 304 1034 1024 16420 634976512.0\n",
      "epoch 305 718 1024 11568 660224128.0\n",
      "epoch 306 1035 1024 16428 266465568.0\n",
      "epoch 307 1044 1024 16496 221083536.0\n",
      "epoch 308 1070 1024 16780 90208208.0\n",
      "epoch 309 1563 2048 27836 103258216.0\n",
      "epoch 310 1456 2048 26200 267779008.0\n",
      "epoch 311 1978 2048 35584 224934816.0\n",
      "epoch 312 1934 2048 35132 374203456.0\n",
      "epoch 313 1031 1024 16408 454036736.0\n",
      "epoch 314 1008 1024 16044 287309184.0\n",
      "epoch 315 1735 2048 31980 373131648.0\n",
      "epoch 316 1530 2048 27468 251528608.0\n",
      "epoch 317 1566 2048 27852 231462176.0\n",
      "epoch 318 1305 2048 23716 207436992.0\n",
      "epoch 319 1931 2048 35116 260494304.0\n",
      "epoch 320 1057 1024 16688 292870912.0\n",
      "epoch 321 591 512 8012 119637520.0\n",
      "epoch 322 1960 2048 35428 214510528.0\n",
      "epoch 323 1024 1024 16332 161367408.0\n",
      "epoch 324 1848 2048 32248 181811488.0\n",
      "epoch 325 801 1024 12504 143972352.0\n",
      "epoch 326 1644 2048 28808 207616048.0\n",
      "epoch 327 2062 2048 36940 382617376.0\n",
      "epoch 328 786 512 11368 247334592.0\n",
      "epoch 329 909 1024 14136 179263936.0\n",
      "epoch 330 2601 4096 52584 582075904.0\n",
      "epoch 331 1607 2048 28444 243285568.0\n",
      "epoch 332 1026 1024 16368 337013344.0\n",
      "epoch 333 793 1024 12448 211949280.0\n",
      "epoch 334 1804 2048 32828 350265344.0\n",
      "epoch 335 1040 1024 16476 204966608.0\n",
      "epoch 336 749 1024 11816 190314736.0\n",
      "epoch 337 2003 2048 36196 209520736.0\n",
      "epoch 338 1559 2048 27804 521205760.0\n",
      "epoch 339 1062 1024 16668 140987216.0\n",
      "epoch 340 1934 2048 35144 461262016.0\n",
      "epoch 341 985 1024 15740 232031328.0\n",
      "epoch 342 1950 2048 35276 211934592.0\n",
      "epoch 343 1052 1024 16624 195979168.0\n",
      "epoch 344 1115 1024 17596 69029264.0\n",
      "epoch 345 1824 2048 32984 156314432.0\n",
      "epoch 346 1033 1024 16416 177405248.0\n",
      "epoch 347 1913 2048 34912 256653664.0\n",
      "epoch 348 1571 2048 28060 194873536.0\n",
      "epoch 349 2332 4096 48316 1277767040.0\n",
      "epoch 350 1642 2048 29156 397757408.0\n",
      "epoch 351 1751 2048 32124 423803584.0\n",
      "epoch 352 1810 2048 32888 380908704.0\n",
      "epoch 353 1462 2048 26672 695618816.0\n",
      "epoch 354 2076 2048 37124 468735424.0\n",
      "epoch 355 491 512 6732 317146432.0\n",
      "epoch 356 1941 2048 35192 230540832.0\n",
      "epoch 357 1869 2048 33608 302149792.0\n",
      "epoch 358 1593 2048 28152 215954400.0\n",
      "epoch 359 1146 1024 17668 192476160.0\n",
      "epoch 360 1942 2048 35224 400299776.0\n",
      "epoch 361 1701 2048 31224 341385792.0\n",
      "epoch 362 1968 2048 35480 405740416.0\n",
      "epoch 363 669 512 8920 362857536.0\n",
      "epoch 364 1034 1024 16340 105358608.0\n",
      "epoch 365 1116 1024 17352 91910256.0\n",
      "epoch 366 2479 4096 50828 713065792.0\n",
      "epoch 367 1557 2048 27768 345688000.0\n",
      "epoch 368 2013 2048 36352 533782720.0\n",
      "epoch 369 1911 2048 34780 825788544.0\n",
      "epoch 370 2082 2048 37192 417698656.0\n",
      "epoch 371 987 1024 15848 787738880.0\n",
      "epoch 372 929 1024 14808 383066144.0\n",
      "epoch 373 1012 1024 15800 262491088.0\n",
      "epoch 374 2049 2048 36824 375904736.0\n",
      "epoch 375 1335 2048 24028 283632768.0\n",
      "epoch 376 2973 4096 59904 1267641600.0\n",
      "epoch 377 2041 2048 36768 756849536.0\n",
      "epoch 378 978 1024 15644 646161792.0\n",
      "epoch 379 1807 2048 32844 680464576.0\n",
      "epoch 380 2076 2048 37148 458128160.0\n",
      "epoch 381 2042 2048 36776 647234240.0\n",
      "epoch 382 1821 2048 32968 493489728.0\n",
      "epoch 383 2057 2048 36904 750188928.0\n",
      "epoch 384 1411 2048 25812 615043072.0\n",
      "epoch 385 1121 1024 17436 408206112.0\n",
      "epoch 386 1045 1024 16504 361168512.0\n",
      "epoch 387 1773 2048 32508 419913280.0\n",
      "epoch 388 1031 1024 16380 184529168.0\n",
      "epoch 389 2029 2048 36488 384088128.0\n",
      "epoch 390 1486 2048 26696 442879552.0\n",
      "epoch 391 1561 2048 27816 219023824.0\n",
      "epoch 392 1530 2048 26628 550712768.0\n",
      "epoch 393 1018 1024 16204 302782816.0\n",
      "epoch 394 1551 2048 27724 439789760.0\n",
      "epoch 395 1186 1024 18448 179937680.0\n",
      "epoch 396 1438 2048 26056 299799232.0\n",
      "epoch 397 2691 4096 54024 1825426176.0\n",
      "epoch 398 1486 2048 26944 1352022528.0\n",
      "epoch 399 1682 2048 31064 825145984.0\n",
      "epoch 400 929 1024 14804 677514816.0\n",
      "epoch 401 785 1024 12368 291688256.0\n",
      "epoch 402 1070 1024 16780 149246176.0\n",
      "epoch 403 989 1024 15616 146873312.0\n",
      "epoch 404 1081 1024 16888 128556384.0\n",
      "epoch 405 1037 1024 16392 122039208.0\n",
      "epoch 406 1569 2048 27896 152720544.0\n",
      "epoch 407 543 512 7372 121953280.0\n",
      "epoch 408 326 256 3672 56412200.0\n",
      "epoch 409 1004 1024 15744 16430278.0\n",
      "epoch 410 1131 1024 17540 51575936.0\n",
      "epoch 411 1048 1024 16604 91724032.0\n",
      "epoch 412 1879 2048 34456 489354816.0\n",
      "epoch 413 1090 1024 17144 369225792.0\n",
      "epoch 414 1787 2048 32604 352714624.0\n",
      "epoch 415 1076 1024 16824 310026944.0\n",
      "epoch 416 1113 1024 17576 230906944.0\n",
      "epoch 417 801 1024 12504 202086800.0\n",
      "epoch 418 1074 1024 16828 93244072.0\n",
      "epoch 419 1042 1024 16476 114889272.0\n",
      "epoch 420 1070 1024 16780 133515672.0\n",
      "epoch 421 1555 2048 27748 154404128.0\n",
      "epoch 422 1968 2048 35480 169851072.0\n",
      "epoch 423 1050 1024 16556 165665376.0\n",
      "epoch 424 1281 1024 19720 350904768.0\n",
      "epoch 425 980 1024 15488 133827336.0\n",
      "epoch 426 1130 1024 17532 157097712.0\n",
      "epoch 427 1041 1024 16472 63506560.0\n",
      "epoch 428 1666 2048 29236 119962992.0\n",
      "epoch 429 1988 2048 36076 279964224.0\n",
      "epoch 430 1959 2048 35420 239639536.0\n",
      "epoch 431 1297 1024 21592 333935072.0\n",
      "epoch 432 1373 2048 25264 330526848.0\n",
      "epoch 433 1022 1024 16320 212205120.0\n",
      "epoch 434 2045 2048 36744 248450208.0\n",
      "epoch 435 1108 1024 17176 323634112.0\n",
      "epoch 436 960 1024 15500 211108576.0\n",
      "epoch 437 1563 2048 27824 129409800.0\n",
      "epoch 438 1934 2048 35132 509822976.0\n",
      "epoch 439 2061 2048 36936 235051088.0\n",
      "epoch 440 1047 1024 16540 208594848.0\n",
      "epoch 441 1071 1024 16796 135006496.0\n",
      "epoch 442 2323 4096 48220 837909248.0\n",
      "epoch 443 1864 2048 33460 425109824.0\n",
      "epoch 444 1944 2048 35236 630977216.0\n",
      "epoch 445 2082 2048 37192 321654400.0\n",
      "epoch 446 2044 2048 36764 557074112.0\n",
      "epoch 447 1681 2048 31056 659146752.0\n",
      "epoch 448 1199 1024 18596 388382144.0\n",
      "epoch 449 794 1024 12456 477815200.0\n",
      "epoch 450 1098 1024 17196 238113056.0\n",
      "epoch 451 1991 2048 36092 259613504.0\n",
      "epoch 452 280 256 3196 168937696.0\n",
      "epoch 453 1225 1024 19000 83819544.0\n",
      "epoch 454 1585 2048 28184 226729232.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 455 1792 2048 32684 210591104.0\n",
      "epoch 456 964 1024 15520 219404128.0\n",
      "epoch 457 1037 1024 16324 77722848.0\n",
      "epoch 458 1850 2048 32260 248639008.0\n",
      "epoch 459 2971 4096 59812 1163210752.0\n",
      "epoch 460 953 1024 15080 585645440.0\n",
      "epoch 461 1545 2048 27680 788963904.0\n",
      "epoch 462 1529 2048 27128 581698560.0\n",
      "epoch 463 2005 2048 36240 486929440.0\n",
      "epoch 464 2324 2048 41172 468842688.0\n",
      "epoch 465 1000 1024 15544 451783200.0\n",
      "epoch 466 1083 1024 16900 127033456.0\n",
      "epoch 467 1191 1024 19036 104174256.0\n",
      "epoch 468 2538 4096 51816 690284544.0\n",
      "epoch 469 1764 2048 32412 726021120.0\n",
      "epoch 470 1828 2048 33040 801590400.0\n",
      "epoch 471 1911 2048 34780 438189568.0\n",
      "epoch 472 1854 2048 33480 786504064.0\n",
      "epoch 473 1232 1024 19544 197428800.0\n",
      "epoch 474 606 512 8140 120423800.0\n",
      "epoch 475 1119 1024 17436 223253472.0\n",
      "epoch 476 1590 2048 28124 128936912.0\n",
      "epoch 477 2055 2048 36768 317257088.0\n",
      "epoch 478 1700 2048 30200 410689728.0\n",
      "epoch 479 1575 2048 27996 526655712.0\n",
      "epoch 480 2035 2048 36652 271640768.0\n",
      "epoch 481 2006 2048 36220 282667520.0\n",
      "epoch 482 1771 2048 32488 945164608.0\n",
      "epoch 483 1988 2048 36008 798179648.0\n",
      "epoch 484 1044 1024 16500 581864448.0\n",
      "epoch 485 1567 2048 27860 267236864.0\n",
      "epoch 486 1653 2048 29144 495126016.0\n",
      "epoch 487 1582 2048 28044 378645184.0\n",
      "epoch 488 2118 2048 37548 281652384.0\n",
      "epoch 489 1081 1024 16888 221757872.0\n",
      "epoch 490 2058 2048 36920 284373824.0\n",
      "epoch 491 1236 1024 19852 224446368.0\n",
      "epoch 492 1565 2048 27848 323783360.0\n",
      "epoch 493 1180 1024 18376 197723488.0\n",
      "epoch 494 1880 2048 34460 259683376.0\n",
      "epoch 495 1822 2048 32972 281628736.0\n",
      "epoch 496 1991 2048 36092 334949952.0\n",
      "epoch 497 1494 2048 26748 1127006464.0\n",
      "epoch 498 1517 2048 27264 393648896.0\n",
      "epoch 499 2759 4096 56556 1160160512.0\n",
      "epoch 500 662 512 9112 650316224.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "def train(model, optimizer, loss):\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, optimizer, loss, True, 0)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "    \n",
    "train(model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 1175 1024 18820 518705088.0\n",
      "epoch 2 956 1024 15108 222067440.0\n",
      "epoch 3 1632 2048 28888 386933568.0\n",
      "epoch 4 1894 2048 33884 526279808.0\n",
      "epoch 5 1092 1024 16984 241735440.0\n",
      "epoch 6 1297 2048 23632 321521472.0\n",
      "epoch 7 1820 2048 32960 194003456.0\n",
      "epoch 8 730 512 9916 220834144.0\n",
      "epoch 9 2052 2048 36864 125004352.0\n",
      "epoch 10 2027 2048 36588 127902992.0\n",
      "epoch 11 1374 1024 23408 310899648.0\n",
      "epoch 12 1860 2048 33524 163150560.0\n",
      "epoch 13 1949 2048 35272 444601536.0\n",
      "epoch 14 840 1024 13084 358040160.0\n",
      "epoch 15 1052 1024 16604 196852928.0\n",
      "epoch 16 2069 2048 37016 142798448.0\n",
      "epoch 17 1559 2048 27804 198553888.0\n",
      "epoch 18 973 1024 15664 319185472.0\n",
      "epoch 19 919 1024 14744 190993552.0\n",
      "epoch 20 1545 2048 27680 195034528.0\n",
      "epoch 21 1502 2048 27068 99031528.0\n",
      "epoch 22 1507 2048 27116 202415152.0\n",
      "epoch 23 1304 1024 21668 467251328.0\n",
      "epoch 24 1058 1024 16680 308212288.0\n",
      "epoch 25 1923 2048 35072 340256192.0\n",
      "epoch 26 1755 2048 32172 363763200.0\n",
      "epoch 27 1047 1024 16512 154744544.0\n",
      "epoch 28 2976 4096 59864 924228160.0\n",
      "epoch 29 1747 2048 32092 690649856.0\n",
      "epoch 30 1943 2048 35204 410156224.0\n",
      "epoch 31 1078 1024 16860 282186624.0\n",
      "epoch 32 2884 4096 57944 1339797248.0\n",
      "epoch 33 2753 4096 56524 2233310208.0\n",
      "epoch 34 2357 4096 49436 3782460672.0\n",
      "epoch 35 1261 2048 23096 1899844608.0\n",
      "epoch 36 1796 2048 32736 1126640896.0\n",
      "epoch 37 1826 2048 33004 1716620032.0\n",
      "epoch 38 1097 1024 17080 869336192.0\n",
      "epoch 39 1565 2048 27848 741850368.0\n",
      "epoch 40 1582 2048 28044 522209408.0\n",
      "epoch 41 1372 1024 21632 592823104.0\n",
      "epoch 42 1283 2048 23508 1126466176.0\n",
      "epoch 43 1863 2048 33564 659133248.0\n",
      "epoch 44 2530 4096 51688 3933432576.0\n",
      "epoch 45 1520 2048 27224 1976646144.0\n",
      "epoch 46 1303 2048 23676 1336908672.0\n",
      "epoch 47 1103 1024 17236 993200960.0\n",
      "epoch 48 1851 2048 33644 798862848.0\n",
      "epoch 49 1529 2048 27488 779149056.0\n",
      "epoch 50 1752 2048 31880 1149429504.0\n",
      "epoch 51 1066 1024 16872 435044992.0\n",
      "epoch 52 827 1024 12796 428193120.0\n",
      "epoch 53 1918 2048 35008 425596384.0\n",
      "epoch 54 1298 2048 23640 372830272.0\n",
      "epoch 55 1101 1024 17248 232020320.0\n",
      "epoch 56 1047 1024 16516 152923344.0\n",
      "epoch 57 3094 4096 61596 870967616.0\n",
      "epoch 58 2042 2048 36728 1744662528.0\n",
      "epoch 59 1753 2048 32160 1442447360.0\n",
      "epoch 60 1812 2048 32884 1191002496.0\n",
      "epoch 61 2160 2048 38424 665175552.0\n",
      "epoch 62 560 512 7512 441797248.0\n",
      "epoch 63 2767 4096 55628 718087936.0\n",
      "epoch 64 1086 1024 16940 295177568.0\n",
      "epoch 65 1859 2048 33784 539922432.0\n",
      "epoch 66 2061 2048 36936 413631616.0\n",
      "epoch 67 1411 2048 25812 941553536.0\n",
      "epoch 68 1756 2048 31936 499768704.0\n",
      "epoch 69 2088 2048 37176 790576384.0\n",
      "epoch 70 1554 2048 27740 480978048.0\n",
      "epoch 71 1029 1024 16312 453174176.0\n",
      "epoch 72 994 1024 15836 191199296.0\n",
      "epoch 73 1881 2048 34440 460632256.0\n",
      "epoch 74 2167 2048 38376 301152544.0\n",
      "epoch 75 1118 1024 17364 381251520.0\n",
      "epoch 76 2041 2048 36768 499454528.0\n",
      "epoch 77 2061 2048 36936 893746496.0\n",
      "epoch 78 1550 2048 27720 720769152.0\n",
      "epoch 79 1066 1024 16700 561187328.0\n",
      "epoch 80 1300 2048 23648 523621984.0\n",
      "epoch 81 1601 2048 28232 501014912.0\n",
      "epoch 82 1027 1024 16348 240114048.0\n",
      "epoch 83 1610 2048 28460 148330400.0\n",
      "epoch 84 928 1024 14860 309217920.0\n",
      "epoch 85 1194 1024 18556 104755360.0\n",
      "epoch 86 2730 4096 55852 1752246784.0\n",
      "epoch 87 2059 2048 36924 736400704.0\n",
      "epoch 88 1697 2048 31192 1106982656.0\n",
      "epoch 89 1090 1024 17032 808197440.0\n",
      "epoch 90 1544 2048 27616 539647680.0\n",
      "epoch 91 1130 1024 17500 226012736.0\n",
      "epoch 92 1533 2048 27580 382932448.0\n",
      "epoch 93 1454 2048 26360 744874112.0\n",
      "epoch 94 2192 2048 39256 282196352.0\n",
      "epoch 95 921 1024 14728 620444288.0\n",
      "epoch 96 1817 2048 31916 297879360.0\n",
      "epoch 97 453 512 6064 275553152.0\n",
      "epoch 98 1596 2048 28168 245760864.0\n",
      "epoch 99 1545 2048 27656 194509200.0\n",
      "epoch 100 999 1024 15708 195335200.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "train(model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 1779 2048 32564 0\n",
      "epoch 2 1800 2048 32740 0\n",
      "epoch 3 1053 1024 16584 0\n",
      "epoch 4 2051 2048 36872 0\n",
      "epoch 5 1043 1024 16484 0\n",
      "epoch 6 1069 1024 16776 0\n",
      "epoch 7 2060 2048 36872 0\n",
      "epoch 8 1989 2048 35676 0\n",
      "epoch 9 2616 4096 52712 0\n",
      "epoch 10 1052 1024 16624 0\n",
      "epoch 11 2071 2048 37028 0\n",
      "epoch 12 920 1024 14720 0\n",
      "epoch 13 1231 1024 19788 0\n",
      "epoch 14 1965 2048 35464 0\n",
      "epoch 15 1810 2048 32860 0\n",
      "epoch 16 1864 2048 33448 0\n",
      "epoch 17 1076 1024 16848 0\n",
      "epoch 18 3606 4096 72836 0\n",
      "epoch 19 2636 4096 52936 0\n",
      "epoch 20 2197 2048 38872 0\n",
      "epoch 21 2030 2048 36244 0\n",
      "epoch 22 2045 2048 36744 0\n",
      "epoch 23 2959 4096 59724 0\n",
      "epoch 24 2063 2048 36972 0\n",
      "epoch 25 3087 4096 61548 0\n",
      "epoch 26 2117 2048 37656 0\n",
      "epoch 27 952 1024 15048 0\n",
      "epoch 28 2060 2048 36920 0\n",
      "epoch 29 1764 2048 32240 0\n",
      "epoch 30 1175 1024 18844 0\n",
      "epoch 31 1838 2048 33164 0\n",
      "epoch 32 2010 2048 36268 0\n",
      "epoch 33 1010 1024 16028 0\n",
      "epoch 34 3049 4096 60780 0\n",
      "epoch 35 1041 1024 16496 0\n",
      "epoch 36 2766 4096 56632 0\n",
      "epoch 37 2058 2048 36920 0\n",
      "epoch 38 898 1024 13636 0\n",
      "epoch 39 1623 2048 28456 0\n",
      "epoch 40 1764 2048 31992 0\n",
      "epoch 41 2042 2048 36716 0\n",
      "epoch 42 2136 2048 37876 0\n",
      "epoch 43 3048 4096 60968 0\n",
      "epoch 44 2820 4096 56332 0\n",
      "epoch 45 2058 2048 36548 0\n",
      "epoch 46 1191 1024 18988 0\n",
      "epoch 47 1782 2048 32472 0\n",
      "epoch 48 1845 2048 33336 0\n",
      "epoch 49 2733 4096 55936 0\n",
      "epoch 50 2021 2048 36348 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "def test(model):\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, None, None, False)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 1847 2048 33308 0\n",
      "epoch 2 2026 2048 36412 0\n",
      "epoch 3 2178 2048 39084 0\n",
      "epoch 4 976 1024 15468 0\n",
      "epoch 5 2961 4096 59736 0\n",
      "epoch 6 1814 2048 32892 0\n",
      "epoch 7 1826 2048 33004 0\n",
      "epoch 8 1942 2048 35212 0\n",
      "epoch 9 2788 4096 56652 0\n",
      "epoch 10 1971 2048 35868 0\n",
      "epoch 11 1342 1024 21140 0\n",
      "epoch 12 1037 1024 16452 0\n",
      "epoch 13 1568 2048 27920 0\n",
      "epoch 14 2046 2048 36748 0\n",
      "epoch 15 1097 1024 17192 0\n",
      "epoch 16 2090 2048 37244 0\n",
      "epoch 17 2082 2048 37108 0\n",
      "epoch 18 1578 2048 28012 0\n",
      "epoch 19 2079 2048 37164 0\n",
      "epoch 20 1456 2048 26260 0\n",
      "epoch 21 1977 2048 35576 0\n",
      "epoch 22 1014 1024 16088 0\n",
      "epoch 23 2986 4096 60012 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "def test(model):\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, None, None, False)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = \"model\"\n",
    "filename = \"model1.pth.tar\"\n",
    "num_epochs = 500\n",
    "\n",
    "def save_model(state, filename='model.pth.tar'):\n",
    "    filename = os.path.join(experiment_dir, filename)\n",
    "    torch.save(state, filename)\n",
    "\n",
    "save_model({\n",
    "    'epoch': num_epochs,\n",
    "    'state_dict': model.cpu().state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
