{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2048(nn.Module):\n",
    "    def __init__(self, input_size=16, filter1=256, filter2=2048, drop_prob=0.):\n",
    "        super(NN2048, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels=input_size, out_channels=input_size, kernel_size=(1,1), padding=0)\n",
    "        self.conv_a = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(2,1), padding=0)\n",
    "        self.conv_a3 = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(3,1), padding=0)\n",
    "        self.conv_a4 = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(4,1), padding=0)\n",
    "        self.conv_b = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(1,2), padding=0)\n",
    "        self.conv_b3 = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(1,3), padding=0)\n",
    "        self.conv_b4 = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(1,4), padding=0)\n",
    "        self.conv_c = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(2,2), padding=0)\n",
    "        \n",
    "        self.conv_aa = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_ab = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        self.conv_ba = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_bb = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        \n",
    "        self.conv_ab3 = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,3), padding=0)\n",
    "        self.conv_ba3 = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(3,1), padding=0)\n",
    "        self.conv_ab4 = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,4), padding=0)\n",
    "        self.conv_ba4 = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(4,1), padding=0)\n",
    "        self.conv_c2 = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,2), padding=0)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.W_aa = nn.Linear(filter2 * 8, 1)\n",
    "        self.W_ab = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_ba = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_bb = nn.Linear(filter2 * 8, 1)\n",
    "        self.W_ab3 = nn.Linear(filter2 * 4, 1)\n",
    "        self.W_ba3 = nn.Linear(filter2 * 4, 1)\n",
    "        self.W_ab4 = nn.Linear(filter2 * 1, 1)\n",
    "        self.W_ba4 = nn.Linear(filter2 * 1, 1)\n",
    "        self.W_c = nn.Linear(filter2 * 4, 1)\n",
    "        self.W_1 = nn.Linear(input_size * 16, 1)\n",
    "\n",
    "    def flatten(self, x):\n",
    "        N = x.size()[0]\n",
    "        return x.view(N, -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        x1 = self.relu(self.conv_1(x))\n",
    "        a = self.relu(self.conv_a(x))\n",
    "        b = self.relu(self.conv_b(x))\n",
    "        c = self.relu(self.conv_c(x))\n",
    "        a3 = self.relu(self.conv_a3(x))\n",
    "        b3 = self.relu(self.conv_b3(x))\n",
    "        a4 = self.relu(self.conv_a4(x))\n",
    "        b4 = self.relu(self.conv_b4(x))\n",
    "        \n",
    "        aa = self.flatten(self.relu(self.conv_aa(a)))\n",
    "        ab = self.flatten(self.relu(self.conv_ab(a)))\n",
    "        ba = self.flatten(self.relu(self.conv_ba(b)))\n",
    "        bb = self.flatten(self.relu(self.conv_bb(b)))\n",
    "        \n",
    "        ab3 = self.flatten(self.relu(self.conv_ab3(a3)))\n",
    "        ba3 = self.flatten(self.relu(self.conv_ba3(b3)))\n",
    "        ab4 = self.flatten(self.relu(self.conv_ab4(a4)))\n",
    "        ba4 = self.flatten(self.relu(self.conv_ba4(b4)))\n",
    "        c2 = self.flatten(self.relu(self.conv_c2(c)))\n",
    "        \n",
    "        out = self.W_aa(aa) + self.W_ab(ab) + self.W_ba(ba) + self.W_bb(bb) + \\\n",
    "              self.W_ab4(ab4) + self.W_ba4(ba4) + self.W_c(c2) + \\\n",
    "              self.W_ab3(ab3) + self.W_ba3(ba3) + self.W_1(self.flatten(x1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(grid):\n",
    "    r = np.zeros(shape=(16, 4, 4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            r[grid[i, j],i, j]=1\n",
    "    return r\n",
    "\n",
    "def add_two(mat):\n",
    "    indexs=np.argwhere(mat==0)\n",
    "    index=np.random.randint(0,len(indexs))\n",
    "    mat[tuple(indexs[index])] = 1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleScore=[0,0,4,16,48,128,320,768,1792,4096,9216,20480,45056,98304,212992,458752,983040]\n",
    "moveDict=np.load('move.npy')\n",
    "\n",
    "def move(list):\n",
    "    return moveDict[list[0],list[1],list[2],list[3],:]\n",
    "\n",
    "def lookup(x):\n",
    "    return singleScore[x]\n",
    "\n",
    "lookup = np.vectorize(lookup)\n",
    "\n",
    "def getScore(matrix):\n",
    "    return np.sum(lookup(matrix))\n",
    "\n",
    "def getMove(grid):\n",
    "    board_list = []\n",
    "    for i in range(4):\n",
    "        newGrid=moveGrid(grid, i)\n",
    "        if not isSame(grid,newGrid):\n",
    "            board_list.append((newGrid, i, getScore(newGrid)))\n",
    "    return board_list\n",
    "        \n",
    "def moveGrid(grid,i):\n",
    "    # new=np.zeros((4,4),dtype=np.int)\n",
    "    new = None\n",
    "    if i==0:\n",
    "        # move up\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==1:\n",
    "        # move left\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int)\n",
    "    elif i==2:\n",
    "        # move down\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==3:\n",
    "        # move right\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int)\n",
    "    return new\n",
    "\n",
    "def isSame(grid1,grid2):\n",
    "    return np.all(grid1==grid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vchange(grid, v):\n",
    "    g0 = grid\n",
    "    g1 = g0[:,::-1,:]\n",
    "    g2 = g0[:,:,::-1]\n",
    "    g3 = g2[:,::-1,:]\n",
    "    r0 = grid.swapaxes(1,2)\n",
    "    r1 = r0[:,::-1,:]\n",
    "    r2 = r0[:,:,::-1]\n",
    "    r3 = r2[:,::-1,:]\n",
    "    xtrain = np.array([g0,g1,g2,g3,r0,r1,r2,r3])\n",
    "    ytrain = np.array([v]*8)\n",
    "    return xtrain, ytrain\n",
    "\n",
    "def gen_sample_and_learn(model, optimizer, loss_fn, is_train = False, explorationProb=0.1):\n",
    "    model.eval()\n",
    "    game_len = 0\n",
    "    game_score = 0\n",
    "    last_grid1 = np.zeros((4,4),dtype=np.int)\n",
    "    last_grid1 = add_two(last_grid1)\n",
    "    last_grid2 = make_input(last_grid1)\n",
    "    last_loss = 0\n",
    "\n",
    "    while True:\n",
    "        grid_array = add_two(last_grid1)\n",
    "        board_list = getMove(grid_array)\n",
    "        if board_list:\n",
    "            boards = np.array([make_input(g) for g,m,s in board_list])\n",
    "            p = model(torch.from_numpy(boards).cuda()).flatten().detach()        \n",
    "            game_len += 1\n",
    "            best_v = None\n",
    "            for i, (g,m,s) in enumerate(board_list):\n",
    "                v = (s - game_score) + p[i].item()\n",
    "                if best_v is None or v > best_v:\n",
    "                    best_v = v\n",
    "                    best_score = s\n",
    "                    best_grid1 = board_list[i][0]\n",
    "                    best_grid2 = boards[i]\n",
    "                    \n",
    "        else:\n",
    "            best_v = 0\n",
    "            best_grid1 = None\n",
    "            best_grid2 = None\n",
    "            \n",
    "        if is_train:\n",
    "            x, y = Vchange(last_grid2, best_v)\n",
    "            x = torch.from_numpy(x).cuda()\n",
    "            y = torch.from_numpy(y).unsqueeze(dim=1).cuda().float()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            last_loss = loss.item()\n",
    "            loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), 10.0) #\n",
    "            optimizer.step()\n",
    "            model.eval()\n",
    "#             if game_len % 50 == 0:\n",
    "#                 print (game_len, last_loss)\n",
    "                \n",
    "        if not board_list:\n",
    "            break\n",
    "            \n",
    "        # gibbs sampling or espilon-greedy\n",
    "        if is_train and random.random() < explorationProb:\n",
    "            idx = random.randint(0, len(board_list) - 1)\n",
    "            game_score = board_list[idx][2]\n",
    "            last_grid1 = board_list[idx][0]\n",
    "            last_grid2 = boards[idx]\n",
    "        else:\n",
    "            game_score = best_score\n",
    "            last_grid1 = best_grid1\n",
    "            last_grid2 = best_grid2\n",
    "        \n",
    "    return game_len, 2**grid_array.max(), game_score, last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "beta1 = 0.9\n",
    "\n",
    "model = NN2048().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, 0.999))\n",
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "experiment_dir = \"model\"\n",
    "\n",
    "def save_model(state, filename='model.pth.tar'):\n",
    "    filename = os.path.join(experiment_dir, filename)\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, checkpoint_path):\n",
    "    ckpt_dict = torch.load(checkpoint_path, map_location=\"cuda:0\")\n",
    "\n",
    "    model.load_state_dict(ckpt_dict['state_dict'])\n",
    "    optimizer.load_state_dict(ckpt_dict['optimizer'])\n",
    "    epoch = ckpt_dict['epoch']\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "model, optimizer, epoch = load_model(model, optimizer, \"model/model5_1000.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "def train(model, optimizer, loss, epoch = 0):\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, optimizer, loss, True, 0)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "        if epoch % 500 == 0:\n",
    "            filename = \"model5_\"+str(epoch)+\".pth.tar\"\n",
    "            save_model({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.cpu().state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, filename)\n",
    "            model.cuda()\n",
    "    \n",
    "train(model, optimizer, loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 2026 2048 36412 0\n",
      "epoch 2 2040 2048 36680 0\n",
      "epoch 3 1060 1024 16660 0\n",
      "epoch 4 801 1024 12488 0\n",
      "epoch 5 727 1024 11628 0\n",
      "epoch 6 1042 1024 16476 0\n",
      "epoch 7 796 1024 12476 0\n",
      "epoch 8 925 1024 14280 0\n",
      "epoch 9 1552 2048 27728 0\n",
      "epoch 10 1801 2048 32800 0\n",
      "epoch 11 1547 2048 27692 0\n",
      "epoch 12 1054 1024 16588 0\n",
      "epoch 13 1038 1024 16456 0\n",
      "epoch 14 1031 1024 16408 0\n",
      "epoch 15 1864 2048 33568 0\n",
      "epoch 16 1809 2048 32856 0\n",
      "epoch 17 1822 2048 32972 0\n",
      "epoch 18 1553 2048 27736 0\n",
      "epoch 19 1823 2048 32980 0\n",
      "epoch 20 569 512 7628 0\n",
      "epoch 21 1617 2048 29264 0\n",
      "epoch 22 2048 2048 36760 0\n",
      "epoch 23 1042 1024 16476 0\n",
      "epoch 24 1044 1024 16488 0\n",
      "epoch 25 1553 2048 27760 0\n",
      "epoch 26 1004 1024 15984 0\n",
      "epoch 27 1972 2048 35884 0\n",
      "epoch 28 1041 1024 16472 0\n",
      "epoch 29 2080 2048 37264 0\n",
      "epoch 30 1559 2048 27804 0\n",
      "epoch 31 1488 2048 26956 0\n",
      "epoch 32 1813 2048 32888 0\n",
      "epoch 33 1914 2048 34892 0\n",
      "epoch 34 1544 2048 27648 0\n",
      "epoch 35 1058 1024 16620 0\n",
      "epoch 36 975 1024 15676 0\n",
      "epoch 37 1054 1024 16588 0\n",
      "epoch 38 980 1024 15716 0\n",
      "epoch 39 1149 1024 18056 0\n",
      "epoch 40 604 512 8120 0\n",
      "epoch 41 1683 2048 29580 0\n",
      "epoch 42 1879 2048 33672 0\n",
      "epoch 43 1547 2048 27692 0\n",
      "epoch 44 574 512 7692 0\n",
      "epoch 45 1027 1024 16348 0\n",
      "epoch 46 1819 2048 32944 0\n",
      "epoch 47 2030 2048 36492 0\n",
      "epoch 48 1577 2048 28008 0\n",
      "epoch 49 1022 1024 16140 0\n",
      "epoch 50 1033 1024 16392 0\n",
      "epoch 51 1846 2048 33244 0\n",
      "epoch 52 1895 2048 34732 0\n",
      "epoch 53 1048 1024 16544 0\n",
      "epoch 54 2011 2048 36284 0\n",
      "epoch 55 1039 1024 16460 0\n",
      "epoch 56 1315 2048 23788 0\n",
      "epoch 57 992 1024 15800 0\n",
      "epoch 58 1882 2048 34472 0\n",
      "epoch 59 2798 4096 56908 0\n",
      "epoch 60 1561 2048 27872 0\n",
      "epoch 61 878 1024 13692 0\n",
      "epoch 62 795 512 11440 0\n",
      "epoch 63 1042 1024 16476 0\n",
      "epoch 64 1961 2048 35552 0\n",
      "epoch 65 1593 2048 28140 0\n",
      "epoch 66 1047 1024 16540 0\n",
      "epoch 67 1814 2048 32892 0\n",
      "epoch 68 1527 2048 27452 0\n",
      "epoch 69 1860 2048 33772 0\n",
      "epoch 70 2774 4096 56712 0\n",
      "epoch 71 1747 2048 32092 0\n",
      "epoch 72 1063 1024 16732 0\n",
      "epoch 73 1721 2048 31464 0\n",
      "epoch 74 1109 1024 17288 0\n",
      "epoch 75 1126 1024 17872 0\n",
      "epoch 76 1381 2048 25320 0\n",
      "epoch 77 1140 1024 17624 0\n",
      "epoch 78 1045 1024 16504 0\n",
      "epoch 79 2906 4096 58540 0\n",
      "epoch 80 2061 2048 36936 0\n",
      "epoch 81 1765 2048 32444 0\n",
      "epoch 82 1775 2048 31500 0\n",
      "epoch 83 1822 2048 32948 0\n",
      "epoch 84 1606 2048 28676 0\n",
      "epoch 85 1936 2048 35152 0\n",
      "epoch 86 861 1024 13256 0\n",
      "epoch 87 2057 2048 36904 0\n",
      "epoch 88 2088 2048 37224 0\n",
      "epoch 89 1011 1024 16156 0\n",
      "epoch 90 925 1024 14776 0\n",
      "epoch 91 1987 2048 36012 0\n",
      "epoch 92 538 512 7308 0\n",
      "epoch 93 1053 1024 16572 0\n",
      "epoch 94 1040 1024 16464 0\n",
      "epoch 95 1582 2048 28044 0\n",
      "epoch 96 1042 1024 16476 0\n",
      "epoch 97 1292 1024 20544 0\n",
      "epoch 98 1053 1024 16640 0\n",
      "epoch 99 1023 1024 16268 0\n",
      "epoch 100 1620 2048 28536 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "def test(model):\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, None, None, False)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# experiment_dir = \"model\"\n",
    "# filename = \"model6.pth.tar\"\n",
    "# num_epochs = 1000\n",
    "\n",
    "# def save_model(state, filename='model.pth.tar'):\n",
    "#     filename = os.path.join(experiment_dir, filename)\n",
    "#     torch.save(state, filename)\n",
    "\n",
    "# save_model({\n",
    "#     'epoch': num_epochs,\n",
    "#     'state_dict': model.cpu().state_dict(),\n",
    "#     'optimizer': optimizer.state_dict(),\n",
    "# }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
