{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2048(nn.Module):\n",
    "    def __init__(self, input_size=16, filter1=512, filter2=4096, drop_prob=0.):\n",
    "        super(NN2048, self).__init__()\n",
    "        self.conv_a = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(2,1), padding=0)\n",
    "        self.conv_b = nn.Conv2d(in_channels=input_size, out_channels=filter1, kernel_size=(1,2), padding=0)\n",
    "        self.conv_aa = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_ab = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        self.conv_ba = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(2,1), padding=0)\n",
    "        self.conv_bb = nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=(1,2), padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.W_aa = nn.Linear(filter2 * 8, 1)\n",
    "        self.W_ab = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_ba = nn.Linear(filter2 * 9, 1)\n",
    "        self.W_bb = nn.Linear(filter2 * 8, 1)\n",
    "\n",
    "    def flatten(self, x):\n",
    "        N = x.size()[0]\n",
    "        return x.view(N, -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        a = self.relu(self.conv_a(x))\n",
    "        b = self.relu(self.conv_b(x))\n",
    "        aa = self.flatten(self.relu(self.conv_aa(a)))\n",
    "        ab = self.flatten(self.relu(self.conv_ab(a)))\n",
    "        ba = self.flatten(self.relu(self.conv_ba(b)))\n",
    "        bb = self.flatten(self.relu(self.conv_bb(b)))\n",
    "        out = self.W_aa(aa) + self.W_ab(ab) + self.W_ba(ba) + self.W_bb(bb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(grid):\n",
    "    r = np.zeros(shape=(16, 4, 4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            r[grid[i, j],i, j]=1\n",
    "    return r\n",
    "\n",
    "def add_two(mat):\n",
    "    indexs=np.argwhere(mat==0)\n",
    "    index=np.random.randint(0,len(indexs))\n",
    "    mat[tuple(indexs[index])] = 1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleScore=[0,0,4,16,48,128,320,768,1792,4096,9216,20480,45056,98304,212992,458752,983040]\n",
    "moveDict=np.load('move.npy')\n",
    "\n",
    "def move(list):\n",
    "    return moveDict[list[0],list[1],list[2],list[3],:]\n",
    "\n",
    "def lookup(x):\n",
    "    return singleScore[x]\n",
    "\n",
    "lookup = np.vectorize(lookup)\n",
    "\n",
    "def getScore(matrix):\n",
    "    return np.sum(lookup(matrix))\n",
    "\n",
    "def getMove(grid):\n",
    "    board_list = []\n",
    "    for i in range(4):\n",
    "        newGrid=moveGrid(grid, i)\n",
    "        if not isSame(grid,newGrid):\n",
    "            board_list.append((newGrid, i, getScore(newGrid)))\n",
    "    return board_list\n",
    "        \n",
    "def moveGrid(grid,i):\n",
    "    # new=np.zeros((4,4),dtype=np.int)\n",
    "    new = None\n",
    "    if i==0:\n",
    "        # move up\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==1:\n",
    "        # move left\n",
    "        new = np.stack([move(grid[row,:]) for row in range(4)], axis = 0).astype(int)\n",
    "    elif i==2:\n",
    "        # move down\n",
    "        grid=np.transpose(grid)\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int).T\n",
    "    elif i==3:\n",
    "        # move right\n",
    "        new = np.stack([np.flip(move(np.flip(grid[row,:]))) for row in range(4)], axis = 0).astype(int)\n",
    "    return new\n",
    "\n",
    "def isSame(grid1,grid2):\n",
    "    return np.all(grid1==grid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vchange(grid, v):\n",
    "    g0 = grid\n",
    "    g1 = g0[:,::-1,:]\n",
    "    g2 = g0[:,:,::-1]\n",
    "    g3 = g2[:,::-1,:]\n",
    "    r0 = grid.swapaxes(1,2)\n",
    "    r1 = r0[:,::-1,:]\n",
    "    r2 = r0[:,:,::-1]\n",
    "    r3 = r2[:,::-1,:]\n",
    "    xtrain = np.array([g0,g1,g2,g3,r0,r1,r2,r3])\n",
    "    ytrain = np.array([v]*8)\n",
    "    return xtrain, ytrain\n",
    "\n",
    "def gen_sample_and_learn(model, optimizer, loss_fn, is_train = False, explorationProb=0.1):\n",
    "    model.eval()\n",
    "    game_len = 0\n",
    "    game_score = 0\n",
    "    last_grid1 = np.zeros((4,4),dtype=np.int)\n",
    "    last_grid1 = add_two(last_grid1)\n",
    "    last_grid2 = make_input(last_grid1)\n",
    "    last_loss = 0\n",
    "\n",
    "    while True:\n",
    "        grid_array = add_two(last_grid1)\n",
    "        board_list = getMove(grid_array)\n",
    "        if board_list:\n",
    "            boards = np.array([make_input(g) for g,m,s in board_list])\n",
    "            p = model(torch.from_numpy(boards).cuda()).flatten().detach()        \n",
    "            game_len += 1\n",
    "            best_v = None\n",
    "            for i, (g,m,s) in enumerate(board_list):\n",
    "                v = (s - game_score) + p[i].item()\n",
    "                if best_v is None or v > best_v:\n",
    "                    best_v = v\n",
    "                    best_score = s\n",
    "                    best_grid1 = board_list[i][0]\n",
    "                    best_grid2 = boards[i]\n",
    "                    \n",
    "        else:\n",
    "            best_v = 0\n",
    "            best_grid1 = None\n",
    "            best_grid2 = None\n",
    "            \n",
    "        if is_train:\n",
    "            x, y = Vchange(last_grid2, best_v)\n",
    "            x = torch.from_numpy(x).cuda()\n",
    "            y = torch.from_numpy(y).unsqueeze(dim=1).cuda().float()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y) / 2\n",
    "            last_loss = loss.item()\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            model.eval()\n",
    "#             if game_len % 30 == 0:\n",
    "#                 print (game_len, last_loss)\n",
    "                \n",
    "        if not board_list:\n",
    "            break\n",
    "            \n",
    "        # gibbs sampling or espilon-greedy\n",
    "        if is_train and random.random() < explorationProb:\n",
    "            idx = random.randint(0, len(board_list) - 1)\n",
    "            game_score = board_list[idx][2]\n",
    "            last_grid1 = board_list[idx][0]\n",
    "            last_grid2 = boards[idx]\n",
    "        else:\n",
    "            game_score = best_score\n",
    "            last_grid1 = best_grid1\n",
    "            last_grid2 = best_grid2\n",
    "        \n",
    "    return game_len, 2**grid_array.max(), game_score, last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 142 128 1264 74606952.0\n",
      "epoch 2 109 64 868 10919294.0\n",
      "epoch 3 233 256 2560 6857856.0\n",
      "epoch 4 282 256 3232 4414761.0\n",
      "epoch 5 251 256 2912 3364850.0\n",
      "epoch 6 281 256 3144 2398961.75\n",
      "epoch 7 386 512 5144 5611510.0\n",
      "epoch 8 721 1024 11224 14043992.0\n",
      "epoch 9 460 512 6016 2479032.0\n",
      "epoch 10 484 512 6264 4012723.5\n",
      "epoch 11 637 512 8824 6213203.0\n",
      "epoch 12 587 512 7868 2556449.5\n",
      "epoch 13 620 512 8264 2825159.75\n",
      "epoch 14 603 512 8124 6206273.0\n",
      "epoch 15 384 256 4376 1381938.875\n",
      "epoch 16 325 256 4012 3051944.75\n",
      "epoch 17 824 1024 12744 24837792.0\n",
      "epoch 18 1009 1024 15304 9607723.0\n",
      "epoch 19 473 512 6280 11234986.0\n",
      "epoch 20 620 512 9020 13031000.0\n",
      "epoch 21 641 512 8768 5207760.0\n",
      "epoch 22 424 256 5212 10112334.0\n",
      "epoch 23 812 1024 12668 20906440.0\n",
      "epoch 24 477 512 6216 11283237.0\n",
      "epoch 25 409 256 5000 7158752.0\n",
      "epoch 26 492 512 6524 6097144.0\n",
      "epoch 27 902 1024 14096 48867784.0\n",
      "epoch 28 547 512 7404 13896891.0\n",
      "epoch 29 903 1024 13672 11451199.0\n",
      "epoch 30 365 256 4228 9984680.0\n",
      "epoch 31 997 1024 15672 48513728.0\n",
      "epoch 32 1109 1024 18048 55245492.0\n",
      "epoch 33 288 256 3276 23077850.0\n",
      "epoch 34 288 256 3276 10317959.0\n",
      "epoch 35 496 512 6796 9920421.0\n",
      "epoch 36 746 512 10808 12603301.0\n",
      "epoch 37 298 256 3340 6572882.0\n",
      "epoch 38 874 1024 13372 11451362.0\n",
      "epoch 39 1008 1024 15960 19508044.0\n",
      "epoch 40 800 1024 12496 27167828.0\n",
      "epoch 41 473 512 6528 12794034.0\n",
      "epoch 42 288 256 3276 6145529.5\n",
      "epoch 43 453 512 6112 6150871.5\n",
      "epoch 44 440 512 5852 5783648.0\n",
      "epoch 45 815 1024 12684 22697860.0\n",
      "epoch 46 570 512 7672 8967067.0\n",
      "epoch 47 525 512 7216 9136558.0\n",
      "epoch 48 991 1024 15820 23720320.0\n",
      "epoch 49 982 1024 15764 35793856.0\n",
      "epoch 50 416 256 5072 13544977.0\n",
      "epoch 51 795 1024 12460 24946386.0\n",
      "epoch 52 1017 1024 16224 29991314.0\n",
      "epoch 53 791 1024 12440 30877838.0\n",
      "epoch 54 881 1024 13840 24000622.0\n",
      "epoch 55 314 256 3532 10122942.0\n",
      "epoch 56 411 512 5516 7343481.0\n",
      "epoch 57 938 1024 14948 18972096.0\n",
      "epoch 58 1319 2048 23980 125588688.0\n",
      "epoch 59 594 512 8028 28175824.0\n",
      "epoch 60 450 512 6036 28933792.0\n",
      "epoch 61 572 512 7792 16618142.0\n",
      "epoch 62 787 1024 12380 19360828.0\n",
      "epoch 63 940 1024 14960 27414852.0\n",
      "epoch 64 324 256 3692 12131715.0\n",
      "epoch 65 1428 1024 23948 47244180.0\n",
      "epoch 66 519 512 7176 34802016.0\n",
      "epoch 67 803 1024 12524 24666128.0\n",
      "epoch 68 764 1024 12028 25961708.0\n",
      "epoch 69 560 512 7596 17614216.0\n",
      "epoch 70 560 512 7568 12716945.0\n",
      "epoch 71 530 512 7256 12330249.0\n",
      "epoch 72 550 512 7448 10027820.0\n",
      "epoch 73 1036 1024 16432 21072476.0\n",
      "epoch 74 568 512 7624 13155500.0\n",
      "epoch 75 1064 1024 16736 30143838.0\n",
      "epoch 76 1794 2048 32752 142720640.0\n",
      "epoch 77 920 1024 14724 91592808.0\n",
      "epoch 78 1033 1024 16416 75708320.0\n",
      "epoch 79 269 256 3116 41572072.0\n",
      "epoch 80 819 1024 12744 26810848.0\n",
      "epoch 81 1323 2048 23916 70464616.0\n",
      "epoch 82 788 1024 12384 45070568.0\n",
      "epoch 83 915 1024 14684 53464496.0\n",
      "epoch 84 916 1024 14692 67097348.0\n",
      "epoch 85 1526 2048 27472 238689184.0\n",
      "epoch 86 581 512 8160 108231248.0\n",
      "epoch 87 358 256 4092 58453184.0\n",
      "epoch 88 804 1024 12556 46025564.0\n",
      "epoch 89 1038 1024 16396 48689676.0\n",
      "epoch 90 916 1024 14688 76177312.0\n",
      "epoch 91 535 512 7292 46472160.0\n",
      "epoch 92 1458 2048 26640 231504608.0\n",
      "epoch 93 1049 1024 16552 102775584.0\n",
      "epoch 94 851 1024 13404 96097072.0\n",
      "epoch 95 548 512 7456 49417944.0\n",
      "epoch 96 1677 2048 31024 135499936.0\n",
      "epoch 97 1309 2048 23720 145599840.0\n",
      "epoch 98 1387 2048 24700 144718416.0\n",
      "epoch 99 985 1024 15776 125209600.0\n",
      "epoch 100 1056 1024 16600 114982584.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "def train(model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.5, 0.999))\n",
    "    loss=nn.MSELoss()\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, optimizer, loss, True, 0)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "    \n",
    "model = NN2048().cuda()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 1321 2048 23880 0\n",
      "epoch 2 1298 2048 23640 0\n",
      "epoch 3 1544 2048 27648 0\n",
      "epoch 4 1051 1024 16572 0\n",
      "epoch 5 1009 1024 16024 0\n",
      "epoch 6 1451 2048 26220 0\n",
      "epoch 7 420 512 5604 0\n",
      "epoch 8 1056 1024 16684 0\n",
      "epoch 9 1819 2048 32948 0\n",
      "epoch 10 1040 1024 16464 0\n",
      "epoch 11 1078 1024 16864 0\n",
      "epoch 12 1086 1024 16972 0\n",
      "epoch 13 1922 2048 34984 0\n",
      "epoch 14 1015 1024 16216 0\n",
      "epoch 15 1433 2048 25980 0\n",
      "epoch 16 982 1024 15736 0\n",
      "epoch 17 1558 2048 27800 0\n",
      "epoch 18 1248 1024 19928 0\n",
      "epoch 19 784 1024 12364 0\n",
      "epoch 20 575 512 7696 0\n",
      "epoch 21 1919 2048 35016 0\n",
      "epoch 22 1075 1024 17192 0\n",
      "epoch 23 1048 1024 16520 0\n",
      "epoch 24 795 1024 12460 0\n",
      "epoch 25 743 1024 11772 0\n",
      "epoch 26 1614 2048 28492 0\n",
      "epoch 27 915 1024 14684 0\n",
      "epoch 28 1059 1024 16628 0\n",
      "epoch 29 1026 1024 16344 0\n",
      "epoch 30 528 512 7232 0\n",
      "epoch 31 1035 1024 16428 0\n",
      "epoch 32 1032 1024 16412 0\n",
      "epoch 33 1433 2048 25992 0\n",
      "epoch 34 2035 2048 36472 0\n",
      "epoch 35 1039 1024 16460 0\n",
      "epoch 36 795 1024 12460 0\n",
      "epoch 37 1348 2048 24120 0\n",
      "epoch 38 1020 1024 16300 0\n",
      "epoch 39 986 1024 15756 0\n",
      "epoch 40 786 1024 12388 0\n",
      "epoch 41 807 1024 12548 0\n",
      "epoch 42 928 1024 14800 0\n",
      "epoch 43 821 1024 12728 0\n",
      "epoch 44 790 1024 12408 0\n",
      "epoch 45 1930 2048 35112 0\n",
      "epoch 46 1024 1024 16272 0\n",
      "epoch 47 1049 1024 16552 0\n",
      "epoch 48 1115 1024 17644 0\n",
      "epoch 49 994 1024 15836 0\n",
      "epoch 50 564 512 7544 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "def test(model):\n",
    "    epoch = 0\n",
    "    while epoch != num_epochs:\n",
    "        epoch += 1\n",
    "        game_len, max_score, game_score, last_loss = gen_sample_and_learn(model, None, None, False)\n",
    "        print ('epoch', epoch, game_len, max_score, game_score, last_loss)\n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
